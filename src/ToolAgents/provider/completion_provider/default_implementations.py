import copy
import re
from enum import Enum

from typing import List, Dict, Union, Any, Optional, Generator
import json

import requests
from mistral_common.protocol.instruct.request import ChatCompletionRequest
from mistral_common.tokens.tokenizers.mistral import (
    MistralTokenizer as MistralTokenizerOfficial,
)


from ToolAgents.messages import ToolCallContent, TextContent, ChatMessage, BinaryContent
from ToolAgents.messages.chat_message import (
    BinaryStorageType,
    ToolCallResultContent,
    ChatMessageRole,
)
from ToolAgents.provider.message_converter.message_converter import BaseMessageConverter
from ToolAgents.provider.llm_provider import ProviderSettings, SamplerSetting
from ToolAgents.provider.completion_provider.completion_interfaces import (
    LLMTokenizer,
    LLMToolCallHandler,
    generate_id,
    CompletionEndpoint,
    AsyncCompletionEndpoint,
)

from ToolAgents import FunctionTool


class HuggingFaceTokenizer(LLMTokenizer):
    def __init__(self, huggingface_tokenizer_model: str):
        from transformers import AutoTokenizer

        self.tokenizer = AutoTokenizer.from_pretrained(huggingface_tokenizer_model)

    def apply_template(self, messages: List[Dict[str, str]], tools: List) -> str:
        return self.tokenizer.apply_chat_template(
            conversation=messages,
            tools=tools if tools and len(tools) > 0 else None,
            tokenize=False,
            add_generation_prompt=True,
        )

    def tokenize(self, text: str) -> List[int]:
        return self.tokenizer.encode(text, add_special_tokens=False)

    def get_eos_token_string(self) -> str:
        return self.tokenizer.decode(self.tokenizer.eos_token_id)


class MistralTokenizerVersion(Enum):
    v1 = 0
    v2 = 1
    v3 = 2
    v7 = 3


class MistralTokenizer(LLMTokenizer):

    def __init__(
        self,
        tokenizer_file: str = None,
        tokenizer_version: MistralTokenizerVersion = MistralTokenizerVersion.v7,
    ):
        if tokenizer_file is not None:
            self.tokenizer = MistralTokenizerOfficial.from_file(
                tokenizer_filename=tokenizer_file
            )
        else:
            if tokenizer_version == MistralTokenizerVersion.v1:
                self.tokenizer = MistralTokenizerOfficial.v1()
            elif tokenizer_version == MistralTokenizerVersion.v2:
                self.tokenizer = MistralTokenizerOfficial.v2()
            elif tokenizer_version == MistralTokenizerVersion.v3:
                self.tokenizer = MistralTokenizerOfficial.v3()
            elif tokenizer_version == MistralTokenizerVersion.v7:
                self.tokenizer = MistralTokenizerOfficial.v7()

    def apply_template(self, messages: List[Dict[str, str]], tools: List) -> str:
        request = ChatCompletionRequest(tools=tools, messages=messages)
        tokenized = self.tokenizer.encode_chat_completion(request)
        text = tokenized.text
        text = text.replace("‚ñÅ", " ")[3:]
        text = text.replace("<0x0A>", "\n")
        return text

    def tokenize(self, text: str) -> List[int]:
        return self.tokenizer.instruct_tokenizer.tokenizer.encode(text, False, False)

    def get_eos_token_string(self) -> str:
        return "</s>"


class TemplateToolCallHandler(LLMToolCallHandler):
    """
    A customizable tool call handler that can be configured with different patterns and formats
    for tool call detection, parsing, and message formatting.
    """

    def __init__(
        self,
        tool_call_start: str = "[TOOL_CALLS]",
        tool_call_pattern: str = r'\[\s*{\s*"name":\s*"[^"]+"\s*,\s*"arguments":\s*{[^}]*}\s*}(?:\s*,\s*{\s*"name":\s*"[^"]+"\s*,\s*"arguments":\s*{[^}]*}\s*})*\s*\]',
        tool_name_field: str = "name",
        arguments_field: str = "arguments",
        debug_mode: bool = False,
    ):
        """
        Initialize the template tool call handler with customizable patterns and field names.

        Args:
            tool_call_pattern: Regex pattern to identify tool calls in the response
            tool_name_field: Field name for the tool/function name
            arguments_field: Field name for the arguments
            debug_mode: Enable debug output
        """
        self.tool_call_start = tool_call_start
        self.tool_call_pattern = re.compile(tool_call_pattern, re.DOTALL)
        self.tool_name_field = tool_name_field
        self.arguments_field = arguments_field
        self.debug = debug_mode

    def contains_partial_tool_calls(self, response: str) -> bool:
        if self.tool_call_start in response:
            return True
        return False

    def contains_tool_calls(self, response: str) -> bool:
        """Check if the response contains tool calls using the configured pattern."""
        response = response.replace("\n", "").replace("\t", "").replace("\r", "")
        matches = self.tool_call_pattern.findall(response.strip())
        if not matches:
            return False
        return True

    def parse_tool_calls(
        self, response: str
    ) -> List[Union[ToolCallContent, TextContent]]:
        """Parse tool calls from the response using the configured patterns."""
        if self.debug:
            print(f"Parsing response: {response}", flush=True)

        tool_calls = []
        matches = self.tool_call_pattern.findall(response.strip())

        for match in matches:
            try:
                # Parse the JSON content
                parsed = json.loads(match)
                # Handle both single tool call and array formats
                if isinstance(parsed, list):
                    calls = parsed
                else:
                    calls = [parsed]

                # Create GenericToolCall objects for valid calls
                for call in calls:
                    tool_calls.append(
                        ToolCallContent(
                            tool_call_id=generate_id(length=9),
                            tool_call_name=call[self.tool_name_field],
                            tool_call_arguments=call[self.arguments_field],
                        )
                    )

            except json.JSONDecodeError as e:
                if self.debug:
                    print(f"Failed to parse tool call: {str(e)}", flush=True)
                return [
                    TextContent(
                        content=f"Failed to parse tool call: {str(e)}\n\n{response.strip()}"
                    )
                ]

        return tool_calls


class MistralMessageConverterLlamaCpp(BaseMessageConverter):
    def prepare_request(
        self,
        model: str,
        messages: List[ChatMessage],
        settings: ProviderSettings = None,
        tools: Optional[List[FunctionTool]] = None,
    ) -> Dict[str, Any]:
        other_messages = self.to_provider_format(messages)
        open_ai_tools = [tool.to_openai_tool() for tool in tools] if tools else None

        request_kwargs = settings.to_dict(
            include=["temperature", "top_p", "max_tokens"]
        )
        request_kwargs["model"] = model
        request_kwargs["messages"] = other_messages
        if open_ai_tools and len(open_ai_tools) > 0:
            request_kwargs["tools"] = open_ai_tools
        else:
            if "tool_choice" in request_kwargs:
                request_kwargs.pop("tool_choice")
        return request_kwargs

    def to_provider_format(self, messages: List[ChatMessage]) -> List[Dict[str, Any]]:
        converted_messages = []
        for message in messages:
            role = message.role.value
            if role == ChatMessageRole.Custom.value:
                role = message.additional_information["custom_role_name"]
            new_content = []
            tool_calls = []
            for content in message.content:
                if isinstance(content, TextContent):
                    if len(content.content) > 0:
                        new_content.append({"type": "text", "text": content.content})
                elif isinstance(content, BinaryContent):
                    if (
                        "image" in content.mime_type
                        and content.storage_type == BinaryStorageType.Url
                    ):
                        new_content.append(
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": content.content,
                                },
                            }
                        )
                elif isinstance(content, ToolCallContent):
                    tool_calls.append(
                        {
                            "id": content.tool_call_id,
                            "function": {
                                "name": content.tool_call_name,
                                "arguments": json.dumps(content.tool_call_arguments),
                            },
                            "type": "function",
                        }
                    )
                elif isinstance(content, ToolCallResultContent):
                    converted_messages.append(
                        {
                            "tool_call_id": content.tool_call_id,
                            "role": "tool",
                            "name": content.tool_call_name,
                            "content": content.tool_call_result,
                        }
                    )
            if len(new_content) > 0:
                if len(tool_calls) > 0:
                    converted_messages.append(
                        {"role": role, "content": new_content, "tool_calls": tool_calls}
                    )
                else:
                    if len(new_content) == 1 and new_content[0]["type"] == "text":
                        converted_messages.append(
                            {"role": role, "content": new_content[0]["text"]}
                        )
                    else:
                        converted_messages.append(
                            {"role": role, "content": new_content}
                        )
            elif len(tool_calls) > 0:
                converted_messages.append(
                    {"role": role, "content": "", "tool_calls": tool_calls}
                )
        return converted_messages


from typing import Any
import json


class LlamaCppProviderSettings(ProviderSettings):
    def __init__(self):
        # Define sampler settings with their default and neutral values
        samplers = [
            SamplerSetting.create_sampler_setting("temperature", 1.0, 1.0),
            SamplerSetting.create_sampler_setting("top_k", 0, 0),
            SamplerSetting.create_sampler_setting("top_p", 1.0, 1.0),
            SamplerSetting.create_sampler_setting("min_p", 0.0, 0.0),
            SamplerSetting.create_sampler_setting("tfs_z", 1.0, 1.0),
            SamplerSetting.create_sampler_setting("typical_p", 1.0, 1.0),
        ]

        # Initialize base class with empty tool choice and samplers
        super().__init__(initial_tool_choice="", samplers=samplers)

        # Initialize other default settings
        self.set_extra_request_kwargs(
            n_keep=0,
            repeat_penalty=1.1,
            repeat_last_n=256,
            penalize_nl=False,
            presence_penalty=0.0,
            frequency_penalty=0.0,
            penalty_prompt=None,
            mirostat=0,
            mirostat_tau=5.0,
            mirostat_eta=0.1,
            cache_prompt=True,
            seed=-1,
            ignore_eos=False,
        )

    def __getattr__(self, name):
        super().__getattr__(name)

    def __setattr__(self, name, value):
        super().__setattr__(name, value)

    def to_dict(
        self, include: list[str] = None, filter_out: list[str] = None
    ) -> dict[str, Any]:
        """Override to handle the specific requirements of llama.cpp"""
        # Get base dictionary from parent class
        result = super().to_dict(include, filter_out)

        # Rename max_tokens to n_predict if present
        if "max_tokens" in result:
            result["n_predict"] = result.pop("max_tokens")

        # Rename stop_sequences to stop if present
        if "stop_sequences" in result:
            result["stop"] = result.pop("stop_sequences")

        return result

    def set_extra_body(self, extra_body: dict[str, Any]):
        pass


class LlamaCppServer(CompletionEndpoint):

    def __init__(self, server_address: str, api_key: str = None):
        super().__init__()
        self.server_address = server_address
        self.api_key = api_key
        self.server_completion_endpoint = f"{server_address}/completion"

    def create_completion(self, prompt: str, settings: LlamaCppProviderSettings):
        headers = self._get_headers()
        data = self._prepare_data(settings, prompt=prompt)
        data["stream"] = False
        response = requests.post(
            self.server_completion_endpoint, headers=headers, json=data
        )
        data = response.json()
        return data["content"]

    def create_streaming_completion(
        self, prompt, settings: ProviderSettings
    ) -> Union[str, Generator[str, None, None]]:
        headers = self._get_headers()
        data = self._prepare_data(settings, prompt=prompt)
        data["stream"] = True
        return self._get_response_stream(headers, data, self.server_completion_endpoint)

    def get_default_settings(self):
        return LlamaCppProviderSettings()

    def _get_headers(self):
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        return headers

    def _prepare_data(self, settings, **kwargs):
        data = settings.to_dict()
        data.update(kwargs)

        # Set default samplers if not provided
        if "samplers" not in data or data["samplers"] is None:
            data["samplers"] = [
                "top_k",
                "tfs_z",
                "typical_p",
                "top_p",
                "min_p",
                "temperature",
            ]

        return data

    def _get_response_stream(self, headers, data, endpoint_address):
        response = requests.post(
            endpoint_address, headers=headers, json=data, stream=True
        )
        response.raise_for_status()

        def generate_text_chunks():
            decoded_chunk = ""
            for chunk in response.iter_lines():
                if chunk:
                    decoded_chunk += chunk.decode("utf-8")
                    if decoded_chunk.strip().startswith("error:"):
                        raise RuntimeError(decoded_chunk)
                    new_data = json.loads(decoded_chunk.replace("data:", ""))
                    returned_data = new_data["content"]
                    yield returned_data
                    decoded_chunk = ""

        return generate_text_chunks()


import json
import aiohttp
import requests
from typing import AsyncGenerator, Dict, Any


class AsyncLlamaCppServer(AsyncCompletionEndpoint):
    def __init__(self, server_address: str, api_key: str = None):
        super().__init__()
        self.server_address = server_address
        self.api_key = api_key
        self.server_completion_endpoint = f"{server_address}/completion"

    async def create_completion(
        self, prompt: str, settings: LlamaCppProviderSettings
    ) -> str:
        """
        Async completion implementation
        """
        headers = self._get_headers()
        data = self._prepare_data(settings, prompt=prompt)
        data["stream"] = False

        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.server_completion_endpoint, headers=headers, json=data
            ) as response:
                response.raise_for_status()
                data = await response.json()
                return data["content"]

    async def create_streaming_completion(
        self, prompt: str, settings: ProviderSettings
    ) -> AsyncGenerator[str, None]:
        """
        Async streaming completion implementation
        """
        headers = self._get_headers()
        data = self._prepare_data(settings, prompt=prompt)
        data["stream"] = True

        async for chunk in self._get_async_response_stream(
            headers, data, self.server_completion_endpoint
        ):
            yield chunk

    def get_default_settings(self) -> LlamaCppProviderSettings:
        return LlamaCppProviderSettings()

    def _get_headers(self) -> Dict[str, str]:
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        return headers

    def _prepare_data(self, settings: ProviderSettings, **kwargs) -> Dict[str, Any]:
        data = settings.to_dict()
        data.update(kwargs)

        # Set default samplers if not provided
        if "samplers" not in data or data["samplers"] is None:
            data["samplers"] = [
                "top_k",
                "tfs_z",
                "typical_p",
                "top_p",
                "min_p",
                "temperature",
            ]

        return data

    async def _get_async_response_stream(
        self, headers: Dict[str, str], data: Dict[str, Any], endpoint_address: str
    ) -> AsyncGenerator[str, None]:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                endpoint_address, headers=headers, json=data
            ) as response:
                response.raise_for_status()

                # Buffer for incomplete chunks
                buffer = ""

                async for chunk in response.content.iter_chunks():
                    if chunk[0]:  # Only process non-empty chunks
                        buffer += chunk[0].decode("utf-8")

                        # Process complete lines in buffer
                        while "\n" in buffer:
                            line, buffer = buffer.split("\n", 1)
                            if line.strip():
                                if line.strip().startswith("error:"):
                                    raise RuntimeError(line)

                                if line.startswith("data:"):
                                    line = line.replace("data:", "").strip()
                                    try:
                                        data = json.loads(line)
                                        yield data["content"]
                                    except json.JSONDecodeError as e:
                                        raise RuntimeError(
                                            f"Failed to parse JSON: {line}"
                                        ) from e

                # Process any remaining data in buffer
                if buffer.strip():
                    if buffer.strip().startswith("error:"):
                        raise RuntimeError(buffer)
                    if buffer.startswith("data:"):
                        buffer = buffer.replace("data:", "").strip()
                        try:
                            data = json.loads(buffer)
                            yield data["content"]
                        except json.JSONDecodeError as e:
                            raise RuntimeError(f"Failed to parse JSON: {buffer}") from e
